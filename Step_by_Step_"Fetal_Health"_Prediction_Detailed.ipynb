{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1553068,
          "sourceType": "datasetVersion",
          "datasetId": 916586
        }
      ],
      "dockerImageVersionId": 30017,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Step by Step \"Fetal Health\" Prediction-Detailed ",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joseph1Otoo/Fetal-Health-Prediction/blob/main/Step_by_Step_%22Fetal_Health%22_Prediction_Detailed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'fetal-health-classification:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F916586%2F1553068%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240609%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240609T225354Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D137eb2b5df705569689ee802ff7b34bf7f1ac2584d8f58dd48c309d19e16a398fefd735f4c397dc4930a21666b46381394323fca6ea4f32b96c1f961be43f69dfa7659ef98822fd8604b965e67e17f65678085d653e0e4abacc72ac281263e4ac07b3ced823506bad70db332c117b898d5b0f866af43f1e815baaa27c66d73c4867d5d18c1b125f5742f6acce8a6976ab205b79c2caa851f1066c159220c4d6d578d5c9c61394895f753a39ba3db839456697c61f5584ca5b12513698f16900a14ed8f47ec024969a52137babc30b8feac7845dafa45be3c4ac8da4e49bd91c4dc61bdb656fe2880ca91fb5463b5c389845b92320f66cf56087adfe272e11f29'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "3zMJHr1-NWZ8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Thank you for opening this kernel!*\n",
        "\n",
        "\n",
        "# Predicting Fetal Health Classification:\n",
        "> In this work, we use machine learning for the prediction of fetal health to prevent child and maternal mortality.\n",
        "\n",
        "## PROJECT CONTENT:\n",
        "* Import Necessary Libraries\n",
        "* Data Exploration/ Analysis/ Visualization\n",
        "* Correlation & Correlation Matrix\n",
        "* Predictive Modeling\n",
        "* Confusion Matrix\n",
        "* Precision and Recall\n",
        "* Hyperparameters Tuning\n",
        "\n",
        "## Goal:\n",
        "> The goal of this competition is to predict Fetal Health. We will practice Classification Algorithms to achieve the lowest prediction error.\n",
        "\n",
        "## Machine learning methods:\n",
        "I have applied consolidated methodologies to identify the most suitable machine learning model for the task from a pool of candidate methodologies.\n",
        "I have taken into consideration a pool of four state-of-the-art machine learning models, that are briefly reviewed in the following:\n",
        "* **Logistic Regression (LR)**: is the baseline model in this Kernel.\n",
        "\n",
        "**NOTE: The key advantages of LR are its simplicity, the scalability to very large datasets and the interpretation it provides in terms of how unitary changes in an input feature influence the log-odds of the associated linear parameter. **\n",
        "\n",
        "* **K-nearest neighbors (KNN)**: k-Nearest Neighbor is a memory-based model, where predictions are performed by the similarity of the current sample to k nearest elements in the training set, according to the given distance metric.\n",
        "\n",
        "**NOTE: The key advantage of this method lies in its sheer simplicity, compensated by the difficulties in robustly determining the most appropriate similarity function as well as the choice of the k meta-parameter. **\n",
        "\n",
        "* **Random Forest (RF)**: is a type of ensemble methods in which multiple learning models are combined together to improve generalization.\n",
        "\n",
        "* **Gradient Boosting Machine (GBM)**: is another ensemble method combining a series of weak learners to obtain a stronger predictor.\n",
        "\n",
        "The **rationale** behind this selection of candidate models was to provide reasonable coverage of different methodologies, to achieve the lowest prediction error."
      ],
      "metadata": {
        "id": "571Y2KQtNWZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Libraries and Data Sets."
      ],
      "metadata": {
        "id": "k9C65HvvNWaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import check_output\n",
        "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action =\"ignore\")\n",
        "\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import the necessary packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "\n",
        "# Algorithms\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import linear_model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import learning_curve"
      ],
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "fDMCxyq8NWaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "data = pd.read_csv('../input/fetal-health-classification/fetal_health.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "n0oy-m_LNWaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA) and Analysis\n",
        "In this step we want to get basic information about the data types, columns, null value counts, memory usage, etc. EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task."
      ],
      "metadata": {
        "id": "dRxnlFzoNWaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows of the dataframe.\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ODtPdf4_NWaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Dataset Columns:\\n{data.columns}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "GYg0_t-jNWaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse statically insight of data\n",
        "data.describe().T"
      ],
      "metadata": {
        "trusted": true,
        "id": "5_OmmE8ONWaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataFrame.describe()** method generates descriptive statistics for only numeric values not for categorical values. This method summarizes the central tendency, dispersion, and shape of a dataset’s distribution, excluding NaN values.\n",
        "\n",
        "Now, let's understand the statistics that are generated by the describe() method:\n",
        "* Count tells us the number of NoN-empty rows in a feature. As we can see there are no NoN-empty rows.\n",
        "* Mean tells us the mean value of that feature.\n",
        "* Std tells us the Standard Deviation Value of that feature.\n",
        "* Min tells us the minimum value of that feature.\n",
        "* 25%, 50%, and 75% are the percentile/quartile of each features. This quartile information helps us to detect Outliers.\n",
        "* Max tells us the maximum value of that feature."
      ],
      "metadata": {
        "id": "tOqJWZaKNWaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info(verbose=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DAaYN0YJNWaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataFrame.info()** method prints a concise summary of a DataFrame.This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage."
      ],
      "metadata": {
        "id": "_RUBXCHXNWaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The dataset size: {data.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "uGMTA68SNWaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Property **DataFrame.shape** returns a tuple representing the dimensionality of the DataFrame."
      ],
      "metadata": {
        "id": "WfJ-0l3bNWaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count the missing and null values\n",
        "Here is good to count the **missing** and **null** values.\n",
        "In the case of a real-world dataset, it is very common that some values in the dataset are missing. We represent these missing values as NaN (Not a Number) values. But to build a good machine learning model our dataset should be complete. That’s why we use some imputation techniques to replace the NaN values with some probable values."
      ],
      "metadata": {
        "id": "DuJg5r0rNWaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the missing and null values for dataset fetal healt.\n",
        "miss_values = data.columns[data.isnull().any()]\n",
        "print(f\"Missing values:\\n{data[miss_values].isnull().sum()}\")\n",
        "\n",
        "null_values = data.columns[data.isna().any()]\n",
        "print(f\"Null values:\\n{data[null_values].isna().sum()}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "hjzzioLENWaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the result, In this case, there is *neither null values nor missing values* in the dataset."
      ],
      "metadata": {
        "id": "LhHJmYTjNWaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize missing values (NaN) using Missingno Library:\n",
        "We use **Missingno library** which offers a very nice way to visualize the distribution of NaN values. Missingno is a Python library and compatible with Pandas.\n",
        ""
      ],
      "metadata": {
        "id": "e2L69WWBNWaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bar Chart :\n",
        "This bar chart gives us an idea about how many missing values are there in each column."
      ],
      "metadata": {
        "id": "yXkY3BDrNWaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Null count analysis\n",
        "null_plot = msno.bar(data, color = \"#5F9EA0\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "TcKnLy5GNWaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The **principle** of this challenge is:\n",
        "> Classify fetal health in order to prevent child and maternal mortality. So get the info about the column of \"fetal_health\" which were classified by three expert obstetritians into 3 classes:\n",
        "\n",
        "* Normal\n",
        "* Suspect\n",
        "* Pathological\n",
        "\n",
        "To do so, we're gonna **analyze and visualize** the target column (fetal_health)."
      ],
      "metadata": {
        "id": "T_XqqKn3NWaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyze and visualize the target column (fetal_health)"
      ],
      "metadata": {
        "id": "c9naVKyUNWaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the **describe() method** for our target column to show the descriptive statistics include those that summarize the central tendency, dispersion, and shape of a dataset’s distribution, excluding NaN values."
      ],
      "metadata": {
        "id": "whgPXvK0NWaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"fetal_health\"].describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "d7zyQoWUNWaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualizations of **\"fetal_health\"** column makes easier to understand the fetal state, and visualization also makes it easier to detect patterns of the fetal state (Normal, Suspect, Pathological)."
      ],
      "metadata": {
        "id": "WjpOyWyHNWaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total = data[\"fetal_health\"].sum()\n",
        "normal = total - 471\n",
        "suspect = total - 1831\n",
        "pathological = total - 1950\n",
        "\n",
        "print(data[\"fetal_health\"].value_counts())\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.subplot(121)\n",
        "# sns.countplot(x=\"fetal_health\", data=data)\n",
        "vis_fetal_health = data.fetal_health.value_counts().plot(figsize=(10, 5), kind=\"bar\", color = [\"#5F9EA0\", \"#B0E0E6\", \"#ADD8E6\"])\n",
        "plt.title(\"Fetal health count\")\n",
        "plt.xlabel(\"Fetal helth\")\n",
        "plt.ylabel(\"Cases\")\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "# plt.pie([normal, suspect, pathological], labels=[\"Normal\", \"Suspect\", \"Pathological\"], autopct=\"%1.0f%%\")\n",
        "plt.title(\"Fetal state\")\n",
        "\n",
        "vis_pie_fetal_health = plt.pie([normal, suspect, pathological], labels=[\"Normal\", \"Suspect\", \"Pathological\"], colors = [\"#5F9EA0\", \"#B0E0E6\", \"#ADD8E6\"], autopct=\"%1.0f%%\")\n",
        "plt.title(\"Fetal health count\")\n",
        "plt.xlabel(\"Fetal helth\")\n",
        "plt.ylabel(\"Cases\")\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "aRzv8VB9NWaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data visualizations of \"fetal_health\" column shows us the percentage of fetal health state."
      ],
      "metadata": {
        "id": "haNzO7jmNWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_hist_plot = data.hist(figsize = (20,20), color = \"#5F9EA0\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "xMOPYb4iNWaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The important things we could learn about the above plot is Skewness.  Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. There are three types of skewed distributions. A right (or positive) skewed distribution, left (or negative) skewed distribution, and normal distribution.\n",
        "* A left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That’s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n",
        "* A right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. That’s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n",
        "* The skewness for a normal distribution is zero and looks a bell curve."
      ],
      "metadata": {
        "id": "_Y5rBwdqNWaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation Numeric featurs with output variable(fetal_health)\n",
        "*Correlation & Correlation Matrix*\n",
        "\n",
        "Here, we want to show the correlation between numerical features and the target \"fetal_health\", in order to have a first idea of the connections between features. Just by looking at the heatmap below we can see some features have the dark colors, Those features have high correlation with the target."
      ],
      "metadata": {
        "id": "W8lsRSH3NWaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_data = data.select_dtypes(exclude=\"object\")\n",
        "numeric_corr = numeric_data.corr()\n",
        "f,ax=plt.subplots(figsize=(25,1))\n",
        "sns.heatmap(numeric_corr.sort_values(by=[\"fetal_health\"], ascending=False).head(1), cmap=\"GnBu\")\n",
        "plt.title(\"Numerical features correlation with the fetal_health\", weight=\"bold\", fontsize=18, color=\"#5F9EA0\")\n",
        "plt.yticks(weight=\"bold\", color=\"darkgreen\", rotation=0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "WhbQa44JNWaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Num_feature = numeric_corr[\"fetal_health\"].sort_values(ascending=False).head(20).to_frame()\n",
        "\n",
        "cm = sns.light_palette(\"#5F9EA0\", as_cmap=True)\n",
        "\n",
        "style = Num_feature.style.background_gradient(cmap=cm)\n",
        "style"
      ],
      "metadata": {
        "trusted": true,
        "id": "V4RgAEerNWaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We can see three features: \"prolongued_decelerations\", \"abnormal_short_term_variability\", \"percentage_of_time_with_abnormal_long_term_variability\" have high correlation with the target culumn (fetal_health)."
      ],
      "metadata": {
        "id": "_vDLvjoxNWaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scatter matrix\n",
        "> A scatter plot matrix is a grid (or matrix) of scatter plots used to visualize bivariate relationships between combinations of variables. Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart."
      ],
      "metadata": {
        "id": "KyxtTGVvNWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "scatterMatrix = scatter_matrix(data,figsize=(50, 50), color = \"#5F9EA0\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "zNFMAPN7NWaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heatmap\n",
        "> A heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information. Correlation heatmaps are ideal for comparing the measurement for each pair of dimension values."
      ],
      "metadata": {
        "id": "UVJF7RSgNWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the size of figure to 12 by 10.\n",
        "plt.figure(figsize=(12,10))\n",
        "\n",
        "# Seaborn has very simple solution for heatmap\n",
        "p=sns.heatmap(data.corr(), annot=True, cmap = \"GnBu\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "HCDkG3ewNWaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling the data\n",
        "> Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one. The most common techniques of feature scaling are Normalization and Standardization. Normalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless. Refer to the below diagram, which shows how data looks after scaling in the X-Y plane.\n",
        "\n",
        "![](https://pariaagharabi.github.io/images/fetal-health-image.png)\n",
        "\n",
        "*NOTE*:\n",
        "> To learn more about scaling techniques: [](http://)https://towardsdatascience.com"
      ],
      "metadata": {
        "id": "P3bnUxnHNWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['baseline value', 'accelerations', 'fetal_movement',\n",
        "       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n",
        "       'prolongued_decelerations', 'abnormal_short_term_variability',\n",
        "       'mean_value_of_short_term_variability',\n",
        "       'percentage_of_time_with_abnormal_long_term_variability',\n",
        "       'mean_value_of_long_term_variability', 'histogram_width',\n",
        "       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n",
        "       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n",
        "       'histogram_median', 'histogram_variance', 'histogram_tendency']\n",
        "scale_X = StandardScaler()\n",
        "X =  pd.DataFrame(scale_X.fit_transform(data.drop([\"fetal_health\"],axis = 1),), columns = columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jtUZV5ChNWaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ClIEQSS1NWaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = data[\"fetal_health\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "EzujSSbtNWaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Train Split and Cross Validation methods\n",
        "* **Train Test Split** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n",
        "* **Cross-validation**, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\n"
      ],
      "metadata": {
        "id": "zlD4PrqvNWaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pCnTxXrJNWaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ],
      "metadata": {
        "trusted": true,
        "id": "Off1lt4SNWaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Machine Learning Models:\n",
        "1. Logistic Regression (LR)\n",
        "2. K-nearest neighbors (KNN)\n",
        "3. Random Forest (RF)\n",
        "4. Gradient Boosting Machine (GBM)\n",
        "\n",
        "NOTE: To improve all scores for each ML model, we want to search the set of \"hyperparameters\" by using the common approach \"Grid search\" for four models above.\n",
        "**Hyper-parameters** are a set of additional, model-dependent parameters that are not inferred automatically by the learning algorithm but need to be specified before the learning phase: a common example of **hyper-parameter** is the value of k in k-Nearest Neighbor or the number of hidden units in a Neural Network.\n",
        "Hence, finding sub-optimal values of the hyper-parameters is crucial to ensure proper generalization. *The hyper-parameter optimization procedure*, which was repeated separately for each candidate learning methodology, encompassed the following steps:\n",
        "* Firstly, a set of suitable hyper-parameters to optimize was identified; for each of them, a range of candidate values was specified. These choices are dependent both on our expertise and on the computational cost needed to train the models.\n",
        "* Secondly, a predictor was learned for all the possible combinations of hyper-parameters and its out-of-sample performance was estimated using 5-fold Cross Validation (CV), i.e.\n",
        "We trained the model with 70% of the total training set size and validated its performance in the remaining 30%.\n",
        "\n",
        "**GridSearch** exhaustively searches through all possible combinations of hyperparameters during training the phase. Before we proceed further, we shall cover another cross-validation (CV) methods since tuning hyperparameters via grid search is usually cross-validated to avoid overfitting.\n",
        "Hence, For accelerating the running GridSearchCV we set: n-splits=3, n_jobs=2."
      ],
      "metadata": {
        "id": "nWTDGTznNWaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "LojyK2fwNWaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (LR)"
      ],
      "metadata": {
        "id": "HRpCSqjINWaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model of Logistic Regression with default parameters:\n",
        "\n",
        "logistic_regression = linear_model.LogisticRegression()\n",
        "logistic_regression_mod = logistic_regression.fit(X_train, y_train)\n",
        "print(f\"Baseline Logistic Regression: {round(logistic_regression_mod.score(X_test, y_test), 3)}\")\n",
        "\n",
        "pred_logistic_regression = logistic_regression_mod.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "0fi4o5QsNWaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here, we are going to tune the baseline model to boost the model."
      ],
      "metadata": {
        "id": "cxlow-AyNWaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_method = StratifiedKFold(n_splits=3,\n",
        "                            random_state=42\n",
        "                            )"
      ],
      "metadata": {
        "trusted": true,
        "id": "qDWu4KCMNWaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validate Logistic Regression model\n",
        "scores_Logistic = cross_val_score(logistic_regression, X_train, y_train, cv =cv_method, n_jobs = 2, scoring = \"accuracy\")\n",
        "\n",
        "print(f\"Scores(Cross validate) for Logistic Regression model:\\n{scores_Logistic}\")\n",
        "print(f\"CrossValMeans: {round(scores_Logistic.mean(), 3)}\")\n",
        "print(f\"CrossValStandard Deviation: {round(scores_Logistic.std(), 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "rAV-THOWNWaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_LR = {\"tol\": [0.0001,0.0002,0.0003],\n",
        "            \"C\": [0.01, 0.1, 1, 10, 100],\n",
        "            \"intercept_scaling\": [1, 2, 3, 4]\n",
        "              }"
      ],
      "metadata": {
        "trusted": true,
        "id": "GSm0tbQ7NWaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(),\n",
        "                                param_grid=params_LR,\n",
        "                                cv=cv_method,\n",
        "                                verbose=1,\n",
        "                                n_jobs=2,\n",
        "                                scoring=\"accuracy\",\n",
        "                                return_train_score=True\n",
        "                                )"
      ],
      "metadata": {
        "trusted": true,
        "id": "04fV1S4vNWaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model with train data\n",
        "GridSearchCV_LR.fit(X_train, y_train);"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZBlBd7RqNWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimator_LR = GridSearchCV_LR.best_estimator_\n",
        "print(f\"Best estimator for LR model:\\n{best_estimator_LR}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "kuT_-n0hNWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_LR = GridSearchCV_LR.best_params_\n",
        "print(f\"Best parameter values for LR model:\\n{best_params_LR}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9kNzRZhINWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best score for LR model: {round(GridSearchCV_LR.best_score_, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "gvm1c-mjNWaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Base on the result above, after tuning our model (LR), We could boost the model just a little bit. So we keep going with other models."
      ],
      "metadata": {
        "id": "dofqlNP4NWaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The grid search returns the following as the best parameter set\n",
        "logistic_regression = linear_model.LogisticRegression(C=10, intercept_scaling=1, tol=0.0001, penalty=\"l2\", solver=\"liblinear\", random_state=42)\n",
        "logistic_regression_mod = logistic_regression.fit(X_train, y_train)\n",
        "pred_logistic_regression = logistic_regression_mod.predict(X_test)\n",
        "\n",
        "mse_logistic_regression = mean_squared_error(y_test, pred_logistic_regression)\n",
        "rmse_logistic_regression = np.sqrt(mean_squared_error(y_test, pred_logistic_regression))\n",
        "score_logistic_regression_train = logistic_regression_mod.score(X_train, y_train)\n",
        "score_logistic_regression_test = logistic_regression_mod.score(X_test, y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tX2H2ytfNWaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Square Error for Logistic Regression = {round(mse_logistic_regression, 3)}\")\n",
        "print(f\"Root Mean Square Error for Logistic Regression = {round(rmse_logistic_regression, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on training set = {round(score_logistic_regression_train, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on testing set = {round(score_logistic_regression_test, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Y4mv0vslNWaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Analysis"
      ],
      "metadata": {
        "id": "mzibqdEnNWaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Classification Report:** Report which includes Precision, Recall and F1-Score.\n",
        "\n",
        "\n",
        "1. **Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
        "\n",
        "    Precision = TP/TP+FP\n",
        "\n",
        "2. **Recall (Sensitivity)** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.\n",
        "\n",
        "    Recall = TP/TP+FN    \n",
        "\n",
        "3. **F1 score** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it’s better to look at both Precision and Recall.\n",
        "\n",
        "    F1 Score = 2(Recall Precision) / (Recall + Precision)"
      ],
      "metadata": {
        "id": "vTzoF0oeNWaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, pred_logistic_regression))"
      ],
      "metadata": {
        "trusted": true,
        "id": "AM5H8n8WNWaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Confusion Matrix**:\n",
        "The confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n"
      ],
      "metadata": {
        "id": "ZnyYivElNWaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, pred_logistic_regression))"
      ],
      "metadata": {
        "trusted": true,
        "id": "kcB_JgAyNWaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_logistic_regression), annot=True, ax = ax, cmap = \"BuGn\");\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel(\"Predicted labels\");\n",
        "ax.set_ylabel(\"True labels\");\n",
        "ax.set_title(\"Confusion Matrix\");\n",
        "ax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);"
      ],
      "metadata": {
        "trusted": true,
        "id": "iK1JseWENWaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "xv5e8PckNWaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Nearest Neighbors (KNN)"
      ],
      "metadata": {
        "id": "5V6k74CrNWaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model of K-Nearest Neighbors with default parameters:\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn_mod = knn.fit(X_train, y_train)\n",
        "print(f\"Baseline K-Nearest Neighbors: {round(knn_mod.score(X_test, y_test), 3)}\")\n",
        "\n",
        "pred_knn = knn_mod.predict(X_test)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5Y7F1wQ9NWaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Here, we are going to tune the baseline model to boost the model."
      ],
      "metadata": {
        "id": "riEOMOYkNWaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validate K-Nearest Neighbors model\n",
        "cv_method = StratifiedKFold(n_splits=3,\n",
        "                            random_state=42\n",
        "                            )\n",
        "\n",
        "scores_knn = cross_val_score(knn, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n",
        "\n",
        "print(f\"Scores(Cross validate) for K-Nearest Neighbors model:\\n{scores_knn}\")\n",
        "print(f\"CrossValMeans: {round(scores_knn.mean(), 3)}\")\n",
        "print(f\"CrossValStandard Deviation: {round(scores_knn.std(), 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "5SQDrDNONWaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_knn = {\"leaf_size\": list(range(1,30)),\n",
        "              \"n_neighbors\": list(range(1,21)),\n",
        "              \"p\": [1,2]}"
      ],
      "metadata": {
        "trusted": true,
        "id": "ODfioqzeNWaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV_knn = GridSearchCV(estimator=KNeighborsClassifier(),\n",
        "                                param_grid=params_knn,\n",
        "                                cv=cv_method,\n",
        "                                verbose=1,\n",
        "                                n_jobs=-1,\n",
        "                                scoring=\"accuracy\",\n",
        "                                return_train_score=True\n",
        "                                )"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q8xTykmSNWaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model with train data\n",
        "GridSearchCV_knn.fit(X_train, y_train);"
      ],
      "metadata": {
        "trusted": true,
        "id": "FuEJmvM4NWaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimator_knn = GridSearchCV_knn.best_estimator_\n",
        "print(f\"Best estimator for KNN model:\\n{best_estimator_knn}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "gtkbB7_yNWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_knn = GridSearchCV_knn.best_params_\n",
        "print(f\"Best parameter values:\\n{best_params_knn}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "FEbOy4jJNWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score_knn = GridSearchCV_knn.best_score_\n",
        "print(f\"Best score for GNB model: {round(best_score_knn, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "mmG4N46sNWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with new parameter for KNN model\n",
        "knn = KNeighborsClassifier(leaf_size=1, n_neighbors=3 , p=1)\n",
        "knn_mod = knn.fit(X_train, y_train)\n",
        "pred_knn = knn_mod.predict(X_test)\n",
        "\n",
        "mse_knn = mean_squared_error(y_test, pred_knn)\n",
        "rmse_knn = np.sqrt(mean_squared_error(y_test, pred_knn))\n",
        "score_knn_train = knn_mod.score(X_train, y_train)\n",
        "score_knn_test = knn_mod.score(X_test, y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "86KqchCXNWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Square Error for K_Nearest Neighbor  = {round(mse_knn, 3)}\")\n",
        "print(f\"Root Mean Square Error for K_Nearest Neighbor = {round(rmse_knn, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on training set = {round(score_knn_train, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on testing set = {round(score_knn_test, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "qyHlzAirNWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, pred_knn))"
      ],
      "metadata": {
        "trusted": true,
        "id": "yg3lTdyONWaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, pred_knn))"
      ],
      "metadata": {
        "trusted": true,
        "id": "h79dV7VZNWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_knn), annot=True, ax = ax, cmap = \"BuGn\");\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel(\"Predicted labels\");\n",
        "ax.set_ylabel(\"True labels\");\n",
        "ax.set_title(\"Confusion Matrix\");\n",
        "ax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);"
      ],
      "metadata": {
        "trusted": true,
        "id": "0wcYGfZENWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "b6iY-dhLNWaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest (RF)"
      ],
      "metadata": {
        "id": "p5dsEUTzNWaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model of RF with default parameters:\n",
        "\n",
        "random_forest = RandomForestClassifier()\n",
        "random_forest_mod = random_forest.fit(X_train, y_train)\n",
        "print(f\"Baseline Random Forest: {round(random_forest_mod.score(X_test, y_test), 3)}\")\n",
        "\n",
        "pred_random_forest = random_forest_mod.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nDD6veSXNWaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validate Random forest model\n",
        "scores_RF = cross_val_score(random_forest, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n",
        "\n",
        "print(f\"Scores(Cross validate) for Random forest model:\\n{scores_RF}\")\n",
        "print(f\"CrossValMeans: {round(scores_RF.mean(), 3)}\")\n",
        "print(f\"CrossValStandard Deviation: {round(scores_RF.std(), 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "GPxV9MjtNWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params_RF = {\"min_samples_split\": [2, 6, 20],\n",
        "              \"min_samples_leaf\": [1, 4, 16],\n",
        "              \"n_estimators\" :[100,200,300,400],\n",
        "              \"criterion\": [\"gini\"]\n",
        "              }"
      ],
      "metadata": {
        "trusted": true,
        "id": "DAaYnigWNWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(),\n",
        "                                param_grid=params_RF,\n",
        "                                cv=cv_method,\n",
        "                                verbose=1,\n",
        "                                n_jobs=2,\n",
        "                                scoring=\"accuracy\",\n",
        "                                return_train_score=True\n",
        "                                )"
      ],
      "metadata": {
        "trusted": true,
        "id": "qCF7sgmMNWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model with train data\n",
        "GridSearchCV_RF.fit(X_train, y_train);"
      ],
      "metadata": {
        "trusted": true,
        "id": "s-w2M8wkNWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_estimator_RF = GridSearchCV_RF.best_estimator_\n",
        "print(f\"Best estimator for RF model:\\n{best_estimator_RF}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4I4PT4kKNWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params_RF = GridSearchCV_RF.best_params_\n",
        "print(f\"Best parameter values for RF model:\\n{best_params_RF}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Sbco8uu2NWaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_score_RF = GridSearchCV_RF.best_score_\n",
        "print(f\"Best score for RF model: {round(best_score_RF, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9ZG2W3YuNWaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_forest = RandomForestClassifier(criterion=\"gini\", n_estimators=100, min_samples_leaf=1, min_samples_split=2, random_state=42)\n",
        "random_forest_mod = random_forest.fit(X_train, y_train)\n",
        "pred_random_forest = random_forest_mod.predict(X_test)\n",
        "\n",
        "mse_random_forest = mean_squared_error(y_test, pred_random_forest)\n",
        "rmse_random_forest = np.sqrt(mean_squared_error(y_test, pred_random_forest))\n",
        "score_random_forest_train = random_forest_mod.score(X_train, y_train)\n",
        "score_random_forest_test = random_forest_mod.score(X_test, y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "NjuXVQ4UNWaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Square Error for Random Forest = {round(mse_random_forest, 3)}\")\n",
        "print(f\"Root Mean Square Error for Random Forest = {round(rmse_random_forest, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Al92Z9cUNWaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, pred_random_forest))"
      ],
      "metadata": {
        "trusted": true,
        "id": "Lmb8GJzdNWaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, pred_random_forest))"
      ],
      "metadata": {
        "trusted": true,
        "id": "P6wx-J3HNWaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_random_forest), annot=True, ax = ax, cmap = \"BuGn\");\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel(\"Predicted labels\");\n",
        "ax.set_ylabel(\"True labels\");\n",
        "ax.set_title(\"Confusion Matrix\");\n",
        "ax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);"
      ],
      "metadata": {
        "trusted": true,
        "id": "90ZtfnEsNWaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br><br>"
      ],
      "metadata": {
        "id": "9FVzoVZ8NWaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting classifier (GBC)\n"
      ],
      "metadata": {
        "id": "MBt4NT-_NWaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline model of gradient boosting classifier with default parameters:\n",
        "gbc = GradientBoostingClassifier()\n",
        "gbc_mod = gbc.fit(X_train, y_train)\n",
        "print(f\"Baseline gradient boosting classifier: {round(gbc_mod.score(X_test, y_test), 3)}\")\n",
        "\n",
        "pred_gbc = gbc_mod.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5YpVSTH-NWaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validate Gradient Boosting Classifier model\n",
        "scores_GBC = cross_val_score(gbc, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n",
        "\n",
        "print(f\"Scores(Cross validate) for Gradient Boosting Classifier model:\\n{scores_GBC}\")\n",
        "print(f\"CrossValMeans: {round(scores_GBC.mean(), 3)}\")\n",
        "print(f\"CrossValStandard Deviation: {round(scores_GBC.std(), 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "pSYH78BrNWaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuning parameters\n",
        "Many strategies exist on how to tune parameters. Most data scientist see **number of trees, tree depth and the learning rate** as most crucial parameters.\n",
        "* **Number of trees**: A high number of trees can be computationally expensive. Generally, with a change in learning rate,n_estimators should also be adjusted (10-fold decrease in learning_rate should go in line with a approx. 10-fold increase in n_estimators.\n",
        "* **Learning rate**: Determines the contribution of each tree on the final outcome and controls how quickly the algorithm proceeds down the gradient descent (learns). This hyperparameter is also called shrinkage. Generally, the smaller this value, the more accurate the model can be but also will require more trees in the sequence.\n",
        "* **Tree depth**: Controls the depth of the individual trees. Typical values range from a depth of 3–8 but it is not uncommon to see a tree depth of 1.\n",
        "\n",
        "Parameter tuning is a crucial task in finding the model with the highest predictive power. The code below how to tune parameters in a gradient boosting model for classification.\n",
        "\n",
        "It's hyperparameter tuning time. First, we need to define a dictionary of GBC parameters for the grid search."
      ],
      "metadata": {
        "id": "v4mmF-oqNWaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params_GBC = {\"loss\": [\"deviance\"],\n",
        "              \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1],\n",
        "              \"n_estimators\": [250, 500],\n",
        "              \"max_depth\": [3, 5, 8]\n",
        "              }"
      ],
      "metadata": {
        "trusted": true,
        "id": "1m1J9--oNWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, we pass the GradientBoostingClassifier() and params_GBC as the model and the parameter dictionary into the GridSearchCV function.\n"
      ],
      "metadata": {
        "id": "k3P_FMBsNWaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GridSearchCV_GBC = GridSearchCV(estimator=GradientBoostingClassifier(),\n",
        "                                param_grid=params_GBC,\n",
        "                                cv=cv_method,\n",
        "                                verbose=1,\n",
        "                                n_jobs=2,\n",
        "                                scoring=\"accuracy\",\n",
        "                                return_train_score=True\n",
        "                                )"
      ],
      "metadata": {
        "trusted": true,
        "id": "1lddsdiBNWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model with train data\n",
        "GridSearchCV_GBC.fit(X_train, y_train);"
      ],
      "metadata": {
        "trusted": true,
        "id": "GVuV4cyYNWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best estimator values.\n",
        "best_estimator_GBC = GridSearchCV_GBC.best_estimator_\n",
        "print(f\"Best estimator values for GBC model:\\n{best_estimator_GBC}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "2ptcjIYUNWaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best parameter values.\n",
        "best_params_GBC = GridSearchCV_GBC.best_params_\n",
        "print(f\"Best parameter values for GBC model:\\n{best_params_GBC}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Wz22gZAqNWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Best score for GBC by using the best_score attribute.\n",
        "best_score_GBC = GridSearchCV_GBC.best_score_\n",
        "print(f\"Best score value foe GBC model: {round(best_score_GBC, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MNC7_0R9NWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with new parameter for GBC model\n",
        "gbc = GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=1, loss=\"deviance\", max_depth=5, max_features=\"log2\", min_samples_leaf=0.2, min_samples_split=0.5, n_estimators=200, random_state=42)\n",
        "gbc_mod = gbc.fit(X_train, y_train)\n",
        "pred_gbc = gbc_mod.predict(X_test)\n",
        "\n",
        "mse_gbc = mean_squared_error(y_test, pred_gbc)\n",
        "rmse_gbc = np.sqrt(mean_squared_error(y_test, pred_gbc))\n",
        "score_gbc_train = gbc_mod.score(X_train, y_train)\n",
        "score_gbc_test = gbc_mod.score(X_test, y_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mLGCrIELNWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Mean Square Error for Gradient Boosting Classifier = {round(mse_gbc, 3)}\")\n",
        "print(f\"Root Mean Square Error for Gradient Boosting Classifier = {round(rmse_gbc, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on training set = {round(score_gbc_train, 3)}\")\n",
        "print(f\"R^2(coefficient of determination) on testing set = {round(score_gbc_test, 3)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "gsB1muZoNWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Classification Report\")\n",
        "print(classification_report(y_test, pred_gbc))"
      ],
      "metadata": {
        "trusted": true,
        "id": "0WPx-TLeNWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, pred_gbc))"
      ],
      "metadata": {
        "trusted": true,
        "id": "wzG1mQKBNWab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(confusion_matrix(y_test, pred_gbc), annot=True, ax = ax, cmap = \"BuGn\");\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel(\"Predicted labels\");\n",
        "ax.set_ylabel(\"True labels\");\n",
        "ax.set_title(\"Confusion Matrix\");\n",
        "ax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);"
      ],
      "metadata": {
        "trusted": true,
        "id": "Gd2PYIVYNWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting the learning curve\n",
        "* Learning curves are plots that show changes in learning performance over time in terms of experience.\n",
        "* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n",
        "* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain."
      ],
      "metadata": {
        "id": "KQbCx4rfNWac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve\n",
        "def plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n",
        "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#80CBC4\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#00897B\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "metadata": {
        "trusted": true,
        "id": "WUFPgiL1NWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Result Visualisation of the learning curve"
      ],
      "metadata": {
        "id": "U3cbYFNtNWac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "plot_learning_curve(GridSearchCV_LR.best_estimator_,title = \"Logistict Regression learning curve\", x = X_train, y = y_train, cv = cv_method);"
      ],
      "metadata": {
        "trusted": true,
        "id": "8-caOKEZNWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_learning_curve(GridSearchCV_knn.best_estimator_,title = \"KNN Classifier learning curve\", x = X_train, y = y_train, cv = cv_method);"
      ],
      "metadata": {
        "trusted": true,
        "id": "r9EN3YDVNWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest\n",
        "plot_learning_curve(GridSearchCV_RF.best_estimator_,title = \"Random Forest learning curve\", x = X_train, y = y_train, cv = cv_method);"
      ],
      "metadata": {
        "trusted": true,
        "id": "iLk92FDQNWac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Classifier\n",
        "plot_learning_curve(GridSearchCV_GBC.best_estimator_,title = \"Gradient Boosting Classifier learning curve\", x = X_train, y = y_train, cv = cv_method);"
      ],
      "metadata": {
        "trusted": true,
        "id": "S64mGa3TNWad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Selection\n"
      ],
      "metadata": {
        "id": "wQ81y6anNWad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = pd.DataFrame({\n",
        "                        \"Model\": [\"Logistic Regression\",\n",
        "                                  \"KNN\",\n",
        "                                  \"Random Forest\",\n",
        "                                  \"Gradient Boosting Classifier\"],\n",
        "                        \"Score\": [logistic_regression_mod.score(X_train, y_train),\n",
        "                                  knn_mod.score(X_train, y_train),\n",
        "                                  random_forest_mod.score(X_train, y_train),\n",
        "                                  gbc_mod.score(X_train, y_train),\n",
        "                                    ]\n",
        "                        })\n",
        "result_df = results.sort_values(by=\"Score\", ascending=False)\n",
        "result_df = result_df.set_index(\"Score\")\n",
        "result_df.head(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Wp4aqLvLNWad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the model selection phase are summarized in Table above. The Random Forest with 0.99 score has high percentage among models. Logistic Regression has lowest score (0.90)."
      ],
      "metadata": {
        "id": "UIj68FHMNWad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thank you for taking the time to read through my kernel. For the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!"
      ],
      "metadata": {
        "id": "GOxpXm-ONWad"
      }
    }
  ]
}